{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uq \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:32.825818Z","iopub.execute_input":"2026-02-03T12:54:32.826197Z","iopub.status.idle":"2026-02-03T12:54:41.872142Z","shell.execute_reply.started":"2026-02-03T12:54:32.826161Z","shell.execute_reply":"2026-02-03T12:54:41.870828Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires google-genai<2.0.0,>=1.45.0, but you have google-genai 1.7.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires google-genai<2.0.0,>=1.37.0, but you have google-genai 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, display\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:41.873568Z","iopub.execute_input":"2026-02-03T12:54:41.873963Z","iopub.status.idle":"2026-02-03T12:54:43.946363Z","shell.execute_reply.started":"2026-02-03T12:54:41.873903Z","shell.execute_reply":"2026-02-03T12:54:43.945354Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nclient = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:43.948587Z","iopub.execute_input":"2026-02-03T12:54:43.949172Z","iopub.status.idle":"2026-02-03T12:54:44.594382Z","shell.execute_reply.started":"2026-02-03T12:54:43.949143Z","shell.execute_reply":"2026-02-03T12:54:44.593076Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n    genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:44.595703Z","iopub.execute_input":"2026-02-03T12:54:44.596089Z","iopub.status.idle":"2026-02-03T12:54:47.920371Z","shell.execute_reply.started":"2026-02-03T12:54:44.596055Z","shell.execute_reply":"2026-02-03T12:54:47.919100Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Evaluation\n\nWe'll evaluate a summarisation task using the Gemini 1.5 Pro technical report.","metadata":{}},{"cell_type":"code","source":"!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n\ndocument_file = client.files.upload(file='gemini.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:47.921923Z","iopub.execute_input":"2026-02-03T12:54:47.922736Z","iopub.status.idle":"2026-02-03T12:54:50.411581Z","shell.execute_reply.started":"2026-02-03T12:54:47.922677Z","shell.execute_reply":"2026-02-03T12:54:50.410273Z"}},"outputs":[{"name":"stdout","text":"2026-02-03 12:54:48 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"#### Summarise a document\nThe summarisation request used here is fairly basic. It targets the training content specifically but provides no guidance otherwise.","metadata":{}},{"cell_type":"code","source":"request = 'Tell me about the training process used here.'\n\ndef summarize_doc(request):\n    \"\"\"Execute the request on the uploaded document.\"\"\"\n    config = types.GenerateContentConfig(temperature=0.0)\n\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=config,\n        contents=[request, document_file]\n    )\n\n    return response.text\n\nsummary = summarize_doc(request)\nMarkdown(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:54:50.412826Z","iopub.execute_input":"2026-02-03T12:54:50.413183Z","iopub.status.idle":"2026-02-03T12:55:04.388908Z","shell.execute_reply.started":"2026-02-03T12:54:50.413150Z","shell.execute_reply":"2026-02-03T12:55:04.387861Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's a breakdown of the training process used for Gemini 1.5 Pro, based on the provided document:\n\n**1. Model Architecture:**\n\n*   **Mixture-of-Experts (MoE):** Gemini 1.5 Pro uses a sparse MoE architecture. This means it has a large number of parameters, but only a subset of them are activated for any given input. A learned routing function directs inputs to a subset of the model's parameters (experts) for processing. This allows for scaling the model's size without a proportional increase in computational cost during inference.\n*   **Transformer-based:** It builds upon the Transformer architecture, which is the foundation for many modern language models.\n\n**2. Training Data:**\n\n*   **Multimodal and Multilingual:** The model is trained on a diverse dataset that includes text, images, audio, and video content. The data is sourced from various domains, including web documents and code.\n*   **Pre-training:** The model undergoes pre-training on this large, diverse dataset.\n*   **Instruction Tuning:** After pre-training, the model is fine-tuned using a collection of multimodal data containing paired instructions and appropriate responses. This helps the model better follow instructions and generate desired outputs.\n*   **Human Preference Data:** Further tuning is based on human preference data, likely using techniques like Reinforcement Learning from Human Feedback (RLHF) to align the model's behavior with human expectations.\n\n**3. Training Infrastructure:**\n\n*   **TPUv4 Accelerators:** The model is trained on multiple 4096-chip pods of Google's TPUv4 accelerators.\n*   **Distributed Training:** The training is distributed across multiple datacenters.\n\n**4. Long-Context Training:**\n\n*   The model incorporates significant architecture changes that enable long-context understanding of inputs up to 10 million tokens without degrading performance.\n\n**5. Safety Mitigations:**\n\n*   **Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF):** These techniques are used to mitigate safety risks.\n*   **Focus on Adversarial Queries:** The safety mitigation is focused on adversarial or \"harm-inducing\" queries, where an unprotected model is likely to produce harmful responses.\n*   **Multimodal Safety Data:** New image-to-text SFT data is incorporated, as text-only safety data was found to be less effective for harm-inducing image-to-text queries.\n\n**In summary, the training process for Gemini 1.5 Pro involves a combination of large-scale pre-training on diverse multimodal data, instruction tuning with paired instructions and responses, and safety mitigations using SFT and RLHF. The model is trained on Google's TPUv4 accelerators using distributed training techniques.**"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"#### Define an evaluator\n\nFor a task like this, we want to evaluate a number of aspects, like how well the model followed the prompt (\"instruction following\"), whether it included relevant data in the prompt (\"groundedness\"), how easy the text is to read (\"fluency\"), or other factors like \"verbosity\" or \"quality\".\n\nIn this step, we define an evaluation agent using a pre-written \"summarisation\" prompt and use it to gauge the quality of the generated summary.","metadata":{}},{"cell_type":"code","source":"SUMMARY_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user input and an AI-generated responses.\nYou should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\nYou will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n\n# Evaluation\n## Metric Definition\nYou will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nConciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\nFluency: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n2: (Bad). The summary is grounded, but does not follow the instructions.\n1: (Very bad). The summary is not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:13.890777Z","iopub.execute_input":"2026-02-03T12:55:13.891127Z","iopub.status.idle":"2026-02-03T12:55:13.896746Z","shell.execute_reply.started":"2026-02-03T12:55:13.891098Z","shell.execute_reply":"2026-02-03T12:55:13.895530Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import enum\n\n# Define a structured enum class to capture the result.\nclass SummaryRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\ndef eval_summary(prompt, ai_response):\n    \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    #generate full chat response\n    response = chat.send_message(\n        message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n    )\n\n    verbose_eval = response.text\n\n    # get desired structure\n    structured_output_config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=SummaryRating,\n        )\n\n    response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n      )\n    structured_eval = response.parsed\n\n    return verbose_eval, structured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:18.043738Z","iopub.execute_input":"2026-02-03T12:55:18.044158Z","iopub.status.idle":"2026-02-03T12:55:18.054032Z","shell.execute_reply.started":"2026-02-03T12:55:18.044128Z","shell.execute_reply":"2026-02-03T12:55:18.052560Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\nMarkdown(text_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:21.912363Z","iopub.execute_input":"2026-02-03T12:55:21.912688Z","iopub.status.idle":"2026-02-03T12:55:23.698988Z","shell.execute_reply.started":"2026-02-03T12:55:21.912664Z","shell.execute_reply":"2026-02-03T12:55:23.697556Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\nSTEP 1:\nThe response contains an overview of the training process from the document, but is not very concise.\n\nSTEP 2:\nI'm giving this response a rating of 4. It mostly follows instructions, is grounded, and is fluent. It is not very concise.\n\n## Rating: 4\n"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:32.391547Z","iopub.execute_input":"2026-02-03T12:55:32.392024Z","iopub.status.idle":"2026-02-03T12:55:32.398610Z","shell.execute_reply.started":"2026-02-03T12:55:32.391994Z","shell.execute_reply":"2026-02-03T12:55:32.397472Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<SummaryRating.GOOD: '4'>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"new_prompt = \"Explain like I'm 5 the training process\"\n\nif not new_prompt:\n  raise ValueError(\"Try setting a new summarisation prompt.\")\n\ndef run_and_eval_summary(prompt):\n  \"\"\"Generate and evaluate the summary using the new prompt.\"\"\"\n  summary = summarize_doc(new_prompt)\n  display(Markdown(summary + '\\n-----'))\n\n  text, struct = eval_summary([new_prompt, document_file], summary)\n  display(Markdown(text + '\\n-----'))\n  print(struct)\n\nrun_and_eval_summary(new_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:38.031274Z","iopub.execute_input":"2026-02-03T12:55:38.031608Z","iopub.status.idle":"2026-02-03T12:55:53.897393Z","shell.execute_reply.started":"2026-02-03T12:55:38.031578Z","shell.execute_reply":"2026-02-03T12:55:53.896267Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can explain the training process of a large language model like Gemini 1.5 in a way that a 5-year-old can understand.\n\nImagine you have a puppy, and you want to teach it to understand and respond to your commands. Here's how it's similar to training a big computer brain:\n\n1.  **Lots and Lots of Examples:**\n    *   The puppy needs to see and hear many examples of what you want it to do. For example, you show it a ball and say \"Fetch!\" over and over.\n    *   The computer brain also needs to see lots of examples. It reads millions of books, articles, and websites. It also sees pictures, videos, and hears sounds. This helps it learn about the world.\n\n2.  **Learning the Rules:**\n    *   The puppy starts to learn that \"Fetch!\" means to go get the ball and bring it back. It learns the rules of the game.\n    *   The computer brain learns the rules of language, like how words go together to make sentences. It also learns about different topics, like animals, planets, and history.\n\n3.  **Practice and Correction:**\n    *   When the puppy does something wrong, you gently correct it. Maybe it brings back a shoe instead of the ball. You say, \"No, fetch the ball!\"\n    *   The computer brain also makes mistakes. When it does, the people who are training it tell it what it did wrong. The computer brain then adjusts itself to do better next time.\n\n4.  **Getting Smarter Over Time:**\n    *   With lots of practice and correction, the puppy gets better and better at understanding what you want it to do.\n    *   The computer brain also gets smarter over time. It can answer questions, write stories, and even translate languages!\n\nSo, training a big computer brain is like teaching a puppy, but with lots and lots more examples and a lot of math! The computer brain learns from all the information it sees and hears, and it gets better and better at understanding and responding to what people want.\n-----"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\nSTEP 1: The response fulfills the prompt, however, it does not provide a summary of the document given. Rather it's a canned answer that it provides given the request to explain a training process like I'm 5.\nSTEP 2: The rating is a 2 as it does not follow instructions, but is grounded in reality.\n\n## Rating\n2\n\n-----"},"metadata":{}},{"name":"stdout","text":"SummaryRating.BAD\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"#### Pointwise evaluation\n\nThe technique used above, where we evaluated a single input/output pair against some criteria is known as pointwise evaluation. This is useful for evaluating singular outputs in an absolute sense, such as \"was it good or bad?\"\n\nIn this exercise, we will try different guidance prompts with a set of questions.","metadata":{}},{"cell_type":"code","source":"terse_guidance = \"Answer the following question in a single sentence, or as close to that as possible.\"\nmoderate_guidance = \"Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\"\ncited_guidance = \"Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\"\nguidance_options = {\n    'Terse': terse_guidance,\n    'Moderate': moderate_guidance,\n    'Cited': cited_guidance,\n}\n\nquestions = [\n    \"What metric(s) are used to evaluate long context performance?\",\n    \"How does the model perform on code tasks?\",\n    \"How many layers does it have?\",\n    # \"Why is it called Gemini?\",\n]\n\nif not questions:\n  raise NotImplementedError('Add some questions to evaluate!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:55:56.814470Z","iopub.execute_input":"2026-02-03T12:55:56.814761Z","iopub.status.idle":"2026-02-03T12:55:56.820635Z","shell.execute_reply.started":"2026-02-03T12:55:56.814738Z","shell.execute_reply":"2026-02-03T12:55:56.819530Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import functools\n\n@functools.cache # caches the output of function in LRU cache (makes fetching results later easy O(1))\ndef answer_question(question, guidance):\n    \"\"\"Generate an answer to the question using the uploaded document and guidance.\"\"\"\n    config = types.GenerateContentConfig(\n        temperature=0.0,\n        system_instruction=guidance\n    )\n\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=config,\n        contents=[question, document_file]\n    )\n\n    return response.text\n\nanswer = answer_question(questions[0], terse_guidance)\nMarkdown(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:03.394221Z","iopub.execute_input":"2026-02-03T12:56:03.394551Z","iopub.status.idle":"2026-02-03T12:56:15.443416Z","shell.execute_reply.started":"2026-02-03T12:56:03.394523Z","shell.execute_reply":"2026-02-03T12:56:15.442466Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Metrics used to evaluate long context performance include next-token prediction, near-perfect retrieval, long-document question answering, long-video question answering, and long-context automatic speech recognition.\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"answer = answer_question(questions[0], cited_guidance)\nMarkdown(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:25.827594Z","iopub.execute_input":"2026-02-03T12:56:25.828028Z","iopub.status.idle":"2026-02-03T12:56:40.837567Z","shell.execute_reply.started":"2026-02-03T12:56:25.827997Z","shell.execute_reply":"2026-02-03T12:56:40.836198Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the document \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", here's a breakdown of the metrics used to evaluate long-context performance:\n\n**1. Diagnostic Long-Context Evaluations:**\n\n*   **Perplexity over Long Sequences:**\n    *   **Metric:** Negative Log-Likelihood (NLL) of tokens at different positions in the input sequences.\n    *   **Interpretation:** A lower NLL indicates better prediction and more effective use of long-context information. The trend of the NLL curve (downward or upward) signifies the model's ability to reason over long contexts.\n*   **Text Haystack (Needle-in-a-Haystack Retrieval):**\n    *   **Metric:** Recall rate (percentage of successful retrievals of the \"needle\" or secret number).\n    *   **Interpretation:** A high recall rate demonstrates the model's ability to reliably retrieve specific information from a large amount of distractor context.\n*   **Video Haystack:**\n    *   **Metric:** Recall rate of the secret word embedded as text in a random frame of a long video.\n    *   **Interpretation:** Tests the model's ability to retrieve specific information across multiple hours of video.\n*   **Audio Haystack:**\n    *   **Metric:** Accuracy (percentage of times the model correctly identifies the \"secret keyword\" in a long audio segment).\n    *   **Interpretation:** Assesses the model's ability to understand audio over long contexts.\n*   **Improved Diagnostics:**\n    *   **Multiple Needles-in-a-Haystack:** Retrieval performance of multiple needles in a single turn.\n    *   **Multi-round Co-reference Resolution (MRCR):** String similarity score between the model output and the correct response in a multi-turn conversation.\n\n**2. Realistic Long-Context Evaluations:**\n\n*   **In-context Language Learning (Machine Translation from One Book - MTOB):**\n    *   **Metrics:**\n        *   Human evaluation scores (on a scale of 0 to 6, with 6 being excellent translation).\n        *   Automatic metrics: BLEURT (for Kalamang to English) and chrF (for English to Kalamang).\n    *   **Interpretation:** Measures the model's ability to learn a new language from a single set of linguistic documentation.\n*   **Long-Document QA:**\n    *   **Metrics:**\n        *   AutoAIS (Automatic Attributable to Identified Sources) score.\n        *   AIS Human Evaluation.\n        *   Number of Sentences per answer.\n    *   **Interpretation:** Assesses the model's ability to answer questions about long documents, requiring understanding of relationships between pieces of information spanning large portions of text.\n*   **Long-Context Audio (Automatic Speech Recognition - ASR):**\n    *   **Metric:** Word Error Rate (WER).\n    *   **Interpretation:** A lower WER indicates better transcription accuracy for long audio segments.\n*   **Long-Context Video QA:**\n    *   **Metric:** Accuracy.\n    *   **Interpretation:** Assesses the model's ability to answer questions about long videos.\n\n**Additional Background and Context:**\n\n*   **Context Window:** The size of the context window (the amount of text, audio, or video the model can consider at once) is a key factor in long-context performance. Gemini 1.5 Pro significantly extends the context length compared to previous models.\n*   **Multimodality:** Gemini 1.5 Pro is a multimodal model, meaning it can process and reason over different types of data (text, audio, video, images) simultaneously.\n*   **Mixture-of-Experts (MoE):** Gemini 1.5 Pro uses a MoE architecture, which allows the model to have a large number of parameters while only activating a subset of them for any given input. This improves efficiency and scalability.\n*   **Retrieval-Augmented Generation:** Some models use external retrieval mechanisms to access relevant information from a database or corpus. Gemini 1.5 Pro's large context window reduces the need for external retrieval in some cases.\n*   **Safety and Responsible Deployment:** The document also discusses the importance of safety and responsible deployment of large language models, including measures to mitigate harmful outputs and biases.\n\nIn summary, the document uses a combination of diagnostic and realistic evaluations to assess Gemini 1.5 Pro's long-context capabilities across different modalities. The metrics used include perplexity, recall, accuracy, human evaluation scores, and automatic metrics like BLEURT and WER.\n"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"Now let's set up a question-answering evaluator, much like before, but using the pointwise QA evaluation prompt.","metadata":{}},{"cell_type":"code","source":"QA_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user prompt and an AI-generated responses.\nYou should first read the user prompt carefully for analyzing the task, and then evaluate the quality of the responses based on and rules provided in the Evaluation section below.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\nYou will assign the writing response a score from 5, 4, 3, 2, 1, following the Rating Rubric and Evaluation Steps.\nGive step-by-step explanations for your scoring, and only choose scores from 5, 4, 3, 2, 1.\n\n## Criteria Definition\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The answer follows instructions, is grounded, complete, and fluent.\n4: (Good). The answer follows instructions, is grounded, complete, but is not very fluent.\n3: (Ok). The answer mostly follows instructions, is grounded, answers the question partially and is not very fluent.\n2: (Bad). The answer does not follow the instructions very well, is incomplete or not fully grounded.\n1: (Very bad). The answer does not follow the instructions, is wrong and not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness,completeness, and fluency according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:48.730899Z","iopub.execute_input":"2026-02-03T12:56:48.731242Z","iopub.status.idle":"2026-02-03T12:56:48.737409Z","shell.execute_reply.started":"2026-02-03T12:56:48.731216Z","shell.execute_reply":"2026-02-03T12:56:48.736186Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class AnswerRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\n\n@functools.cache\ndef eval_answer(prompt, ai_response, n=1):\n    \"\"\"Evaluate the generated answer against the prompt/question used.\"\"\"\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    response = chat.send_message(\n        message=QA_PROMPT.format(prompt=prompt, response=ai_response)\n    )\n    verbose_eval = response.text\n\n    # structured output\n    config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=AnswerRating,\n    )\n\n    response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=config,\n      )\n    structured_eval = response.parsed\n\n    return verbose_eval, structured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:52.902373Z","iopub.execute_input":"2026-02-03T12:56:52.902676Z","iopub.status.idle":"2026-02-03T12:56:52.909972Z","shell.execute_reply.started":"2026-02-03T12:56:52.902653Z","shell.execute_reply":"2026-02-03T12:56:52.908807Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"text_eval, struct_eval = eval_answer(prompt=questions[0], ai_response=answer)\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:56.565607Z","iopub.execute_input":"2026-02-03T12:56:56.565937Z","iopub.status.idle":"2026-02-03T12:56:58.209519Z","shell.execute_reply.started":"2026-02-03T12:56:56.565902Z","shell.execute_reply":"2026-02-03T12:56:58.208461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: The model gives a thorough response about the different metrics that are used to evaluate long context performance. It also includes the interpretation of the metrics and the meaning behind them.\nSTEP 2: The answer follows instructions, is grounded, complete, and fluent.\n\nScore: 5\n"},"metadata":{}},{"name":"stdout","text":"AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:56:59.879562Z","iopub.execute_input":"2026-02-03T12:56:59.879896Z","iopub.status.idle":"2026-02-03T12:56:59.886816Z","shell.execute_reply.started":"2026-02-03T12:56:59.879867Z","shell.execute_reply":"2026-02-03T12:56:59.886024Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<AnswerRating.VERY_GOOD: '5'>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import collections\nimport itertools\n\nNUM_ITERATIONS = 1\n\nscores = collections.defaultdict(int)\nresponses = collections.defaultdict(list)\n\nfor question in questions:\n    display(Markdown(f'## {question}'))\n    for guidance, guide_prompt in guidance_options.items():\n        for n in range(NUM_ITERATIONS):\n            answer = answer_question(question, guide_prompt)\n            written_eval, struct_eval = eval_answer(question, answer, n)\n            print(f'{guidance}: {struct_eval}')\n            scores[guidance] += int(struct_eval.value)\n            responses[(guidance, question)].append((answer, written_eval))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:57:02.243742Z","iopub.execute_input":"2026-02-03T12:57:02.244142Z","iopub.status.idle":"2026-02-03T12:58:34.329194Z","shell.execute_reply.started":"2026-02-03T12:57:02.244114Z","shell.execute_reply":"2026-02-03T12:58:34.328035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## What metric(s) are used to evaluate long context performance?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.VERY_GOOD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How does the model perform on code tasks?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.VERY_GOOD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How many layers does it have?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.OK\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"for guidance, score in scores.items():\n  avg_score = score / (NUM_ITERATIONS * len(questions))\n  nearest = AnswerRating(str(round(avg_score)))\n  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:38.025671Z","iopub.execute_input":"2026-02-03T12:58:38.026072Z","iopub.status.idle":"2026-02-03T12:58:38.032248Z","shell.execute_reply.started":"2026-02-03T12:58:38.026043Z","shell.execute_reply":"2026-02-03T12:58:38.030762Z"}},"outputs":[{"name":"stdout","text":"Terse: 5.00 - VERY_GOOD\nModerate: 4.33 - GOOD\nCited: 5.00 - VERY_GOOD\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"#### Pairwise Evaluation","metadata":{}},{"cell_type":"code","source":"QA_PAIRWISE_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models. We will provide you with the user input and a pair of AI-generated responses (Response A and Response B). You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n\nYou will first judge responses individually, following the Rating Rubric and Evaluation Steps. Then you will give step-by-step explanations for your judgment, compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n\"A\": Response A answers the given question as per the criteria better than response B.\n\"SAME\": Response A and B answers the given question equally well as per the criteria.\n\"B\": Response B answers the given question as per the criteria better than response A.\n\n## Evaluation Steps\nSTEP 1: Analyze Response A based on the question answering quality criteria: Determine how well Response A fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 2: Analyze Response B based on the question answering quality criteria: Determine how well Response B fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\nSTEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\nSTEP 5: Output your assessment reasoning in the explanation field.\n\n# User Inputs and AI-generated Responses\n## User Inputs\n### Prompt\n{prompt}\n\n# AI-generated Response\n\n### Response A\n{baseline_model_response}\n\n### Response B\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:41.836343Z","iopub.execute_input":"2026-02-03T12:58:41.836638Z","iopub.status.idle":"2026-02-03T12:58:41.842722Z","shell.execute_reply.started":"2026-02-03T12:58:41.836607Z","shell.execute_reply":"2026-02-03T12:58:41.841694Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class AnswerComparison(enum.Enum):\n  A = 'A'\n  SAME = 'SAME'\n  B = 'B'\n\n\n@functools.cache\ndef eval_pairwise(prompt, response_a, response_b, n=1):\n    \"\"\"Determine the better of two answers to the same prompt.\"\"\"\n\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    response = chat.send_message(\n        message=QA_PAIRWISE_PROMPT.format(prompt=prompt, baseline_model_response=response_a, response=response_b)\n    )\n    verbose_eval = response.text\n\n    structured_output_config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=AnswerComparison\n    )\n\n    response = chat.send_message(\n        message=\"Convert the final score.\",\n        config=structured_output_config,\n    )\n\n    stuctured_eval = response.parsed\n\n    return verbose_eval, stuctured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:45.357475Z","iopub.execute_input":"2026-02-03T12:58:45.358190Z","iopub.status.idle":"2026-02-03T12:58:45.365004Z","shell.execute_reply.started":"2026-02-03T12:58:45.358156Z","shell.execute_reply":"2026-02-03T12:58:45.364016Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"question = questions[0]\nanswer_a = answer_question(question, terse_guidance)\nanswer_b = answer_question(question, cited_guidance)\n\ntext_eval, struct_eval = eval_pairwise(\n    prompt=question,\n    response_a=answer_a,\n    response_b=answer_b,\n)\n\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:49.694497Z","iopub.execute_input":"2026-02-03T12:58:49.694924Z","iopub.status.idle":"2026-02-03T12:58:51.781550Z","shell.execute_reply.started":"2026-02-03T12:58:49.694896Z","shell.execute_reply":"2026-02-03T12:58:51.780469Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: Analyze Response A based on the question answering quality criteria:\nResponse A provides a list of metrics, but it is too short and doesn't provide enough detail.\n\nSTEP 2: Analyze Response B based on the question answering quality criteria:\nResponse B is very thorough and provides a lot of detail regarding the metrics used to evaluate long context performance.\n\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\nResponse B is much more helpful and provides more detail to the user.\n\nSTEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\nB\n\nSTEP 5: Output your assessment reasoning in the explanation field.\nResponse B provides a comprehensive list of metrics used to evaluate long context performance, and includes useful context for each metric. Response A is too short."},"metadata":{}},{"name":"stdout","text":"AnswerComparison.B\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"@functools.total_ordering\nclass QAGuidancePrompt:\n  \"\"\"A question-answering guidance prompt or system instruction.\"\"\"\n\n  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n    \"\"\"Create the prompt. Provide questions to evaluate against, and number of evals to perform.\"\"\"\n    self.prompt = prompt\n    self.questions = questions\n    self.n = n_comparisons\n\n  def __str__(self):\n    return self.prompt\n\n  def _compare_all(self, other):\n    \"\"\"Compare two prompts on all questions over n trials.\"\"\"\n    results = [self._compare_n(other, q) for q in questions]\n    mean = sum(results) / len(results)\n    return round(mean)\n\n  def _compare_n(self, other, question):\n    \"\"\"Compare two prompts on a question over n trials.\"\"\"\n    results = [self._compare(other, question, n) for n in range(self.n)]\n    mean = sum(results) / len(results)\n    return mean\n\n  def _compare(self, other, question, n=1):\n    \"\"\"Compare two prompts on a single question.\"\"\"\n    answer_a = answer_question(question, self.prompt)\n    answer_b = answer_question(question, other.prompt)\n\n    _, result = eval_pairwise(\n        prompt=question,\n        response_a=answer_a,\n        response_b=answer_b,\n        n=n,  # Cache buster\n    )\n\n    # Convert the enum to the standard Python numeric comparison values.\n    if result is AnswerComparison.A:\n      return 1\n    elif result is AnswerComparison.B:\n      return -1\n    else:\n      return 0\n\n  def __eq__(self, other):\n    \"\"\"Equality check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) == 0\n\n  def __lt__(self, other):\n    \"\"\"Ordering check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) < 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:54.649451Z","iopub.execute_input":"2026-02-03T12:58:54.649792Z","iopub.status.idle":"2026-02-03T12:58:54.659851Z","shell.execute_reply.started":"2026-02-03T12:58:54.649767Z","shell.execute_reply":"2026-02-03T12:58:54.658799Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"Ranking prompts against each-other","metadata":{}},{"cell_type":"code","source":"terse_prompt = QAGuidancePrompt(terse_guidance, questions)\nmoderate_prompt = QAGuidancePrompt(moderate_guidance, questions)\ncited_prompt = QAGuidancePrompt(cited_guidance, questions)\n\n# Sort in reverse order, so that best is first\nsorted_results = sorted([terse_prompt, moderate_prompt, cited_prompt], reverse=True)\nfor i, p in enumerate(sorted_results):\n  if i:\n    print('---')\n\n  print(f'#{i+1}: {p}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:58:58.902457Z","iopub.execute_input":"2026-02-03T12:58:58.902788Z","iopub.status.idle":"2026-02-03T12:59:17.385069Z","shell.execute_reply.started":"2026-02-03T12:58:58.902760Z","shell.execute_reply":"2026-02-03T12:59:17.384162Z"}},"outputs":[{"name":"stdout","text":"#1: Answer the following question in a single sentence, or as close to that as possible.\n---\n#2: Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\n---\n#3: Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}